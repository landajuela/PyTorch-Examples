{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder-Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Import the modules:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch import optim\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We consider the vectors obtained by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$(f_1(t_i),f_2(t_i),f_3(t_i),f_4(t_i),f_5(t_i),f_6(t_i)), \\ i > 0$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(t):\n",
    "    \"f1 function\"\n",
    "    ret = torch.sin(t)\n",
    "    return ret\n",
    "def f2(t):\n",
    "    \"f2 function\"\n",
    "    ret = torch.cos(t)\n",
    "    return ret\n",
    "def f3(t):\n",
    "    \"f3 function\"\n",
    "    ret = t\n",
    "    return ret\n",
    "def f4(t):\n",
    "    \"f4 function\"\n",
    "    ret = f1(t) - f2(t) \n",
    "    return ret\n",
    "def f5(t):\n",
    "    \"f5 function\"\n",
    "    ret = f1(t) - f2(t) + f3(t) \n",
    "    return ret\n",
    "def f6(t):\n",
    "    \"f5 function\"\n",
    "    ret = f1(t) + f2(t) + f3(t) \n",
    "    return ret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = 0.1\n",
    "t = torch.arange(-1.0, 1.0, dt)\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(t.numpy(),f1(t).numpy(),\"-\", label=\"f1\")\n",
    "ax.plot(t.numpy(),f2(t).numpy(), \"-\", label=\"f2\")\n",
    "ax.plot(t.numpy(),f3(t).numpy(), \"-\", label=\"f3\")\n",
    "ax.plot(t.numpy(),f4(t).numpy(), \"-\", label=\"f4\")\n",
    "ax.plot(t.numpy(),f5(t).numpy(), \"-\", label=\"f5\")\n",
    "ax.plot(t.numpy(),f6(t).numpy(), \"-\", label=\"f6\")\n",
    "ax.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Create Pytoch Dataset handler:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class vec2vecDataset(Dataset):\n",
    "    \"\"\" vec2vec dataset.\"\"\"\n",
    "    \n",
    "    def __init__(self, h_size):\n",
    "        \n",
    "        # Generate the data\n",
    "        tt = torch.arange(-1.0, 1.0, h_size)  \n",
    "        lentt = tt.shape[0]        \n",
    "        T1 = f1(tt).view([1,lentt])\n",
    "        T2 = f2(tt).view([1,lentt])\n",
    "        T3 = f3(tt).view([1,lentt])\n",
    "        T4 = f4(tt).view([1,lentt])\n",
    "        T5 = f5(tt).view([1,lentt])\n",
    "        T6 = f6(tt).view([1,lentt])\n",
    "        self.X = torch.cat((T1, T2, T3, T4, T5, T6), 0)\n",
    "        self.Y = self.X \n",
    "        self.len = self.X.shape[1]\n",
    "                \n",
    "    def __getitem__(self, index):\n",
    "        return self.X[:,index], self.Y[:,index]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Inspect the Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = vec2vecDataset(0.05)\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, data in enumerate(train_loader, 0):\n",
    " \n",
    "    print(f'i : {i}')\n",
    "    print('len(data) : {},  data[0].shape : {}, data[1].shape : {}'.format(len(data), data[0].shape, data[1].shape))\n",
    "    # Get the inputs\n",
    "    X, y = data\n",
    "    \n",
    "    fig, ax = plt.subplots()\n",
    "    ax.plot(X[:,:].numpy(),\".\", label=\"data\")\n",
    "    ax.plot(y[:,:].numpy(), \".\", label=\"pred\")\n",
    "    \n",
    "    print('X.shape : {}, y.shape : {}'.format(X.shape, y.shape))\n",
    "    \n",
    "    if i == 0:\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Simple encoder-decoder architecture:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, 2)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(2, 10),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(10, input_dim),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.encoder(x)\n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialitation\n",
    "def init_weights(self):\n",
    "    for idx, m in enumerate(self.modules()):         \n",
    "        if idx > 0:\n",
    "            print('{} -> {}'.format(idx,m))\n",
    "            if type(m) in [nn.Linear]:\n",
    "                for name, param in m.named_parameters():\n",
    "                    print(f'Initialization of {name}', end=\"\", flush=True)\n",
    "                    if 'weight' in name:   \n",
    "                        torch.nn.init.xavier_uniform_(param.data)\n",
    "                        print('...done')\n",
    "                    elif 'bias' in name:\n",
    "                        param.data.fill_(0)\n",
    "                        print('...done') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Setup the training process: Model + Loss + Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data parameters\n",
    "h_size = 0.001\n",
    "\n",
    "# Global parameters\n",
    "input_dim = 6      # input dimension\n",
    "\n",
    "# Dataset\n",
    "dataset = vec2vecDataset(h_size)\n",
    "# Dataloader\n",
    "train_loader = DataLoader(dataset=dataset,\n",
    "                          batch_size=32,\n",
    "                          shuffle=True,\n",
    "                          num_workers=1)\n",
    "\n",
    "# Model\n",
    "model = Autoencoder(input_dim)\n",
    "\n",
    "# Initialization     \n",
    "print('model.modules() : {}\\n'.format(model.modules()))\n",
    "i = 0\n",
    "for m in model.modules():\n",
    "    print('i : {}'.format(i,))\n",
    "    print('m : {} \\ntype(m) : {}'.format(m, type(m)))\n",
    "    print('m.named_parameters() :')\n",
    "    for name, param in m.named_parameters():\n",
    "        print('  name : {}'.format(name))\n",
    "        print('  param.shape : {}'.format(param.shape))  \n",
    "    i = i + 1\n",
    "    print('\\n')    \n",
    "    \n",
    "# Initialization         \n",
    "model.apply(init_weights)\n",
    "\n",
    "# Criterion\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# SGD Optimizer\n",
    "learning_rate = 0.001\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Training:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    for i, data in enumerate(train_loader, 0):\n",
    " \n",
    "        # Get the inputs\n",
    "        X, y = data\n",
    "\n",
    "        # Train step\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        y_ = model(X)\n",
    "        loss = criterion(y_, y)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if epoch % 100 == 0:\n",
    "            if i % 16 == 0:        \n",
    "                # Evaluation\n",
    "                model.eval()\n",
    "                with torch.no_grad():\n",
    "                    y_ = model(X)\n",
    "\n",
    "                # Compute and print loss\n",
    "                loss = criterion(y_, y)\n",
    "                print(f'Epoch : {epoch}, Iteration : {i}, Loss : {loss.item()}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    tt = torch.arange(-1.0, 1.0, h_size)\n",
    "    for i, t in enumerate(tt):\n",
    "        x = torch.tensor([f1(t),f2(t),f3(t),f4(t),f5(t),f6(t)])\n",
    "        x_enc = model.encoder(x)\n",
    "        y = model(x)\n",
    "        x_enc = x_enc.view(1,-1)\n",
    "        y = y.view(1,-1)\n",
    "        if i == 0:\n",
    "            xt_enc = x_enc\n",
    "            yt = y\n",
    "        else:\n",
    "            xt_enc = torch.cat((xt_enc, x_enc), 0)\n",
    "            yt = torch.cat((yt, y), 0)\n",
    "\n",
    "    dt = 0.001\n",
    "    t = torch.arange(-1.0, 1.0, dt)  \n",
    "    plt.figure(figsize = (16, 10))\n",
    "    plt.plot(tt.numpy(),f1(tt).numpy(),\"-b\", label=\"f1\")\n",
    "    plt.plot(tt.numpy(),yt[:,0].numpy(),\"--b\", label=\"f1-pred\")\n",
    "    plt.plot(tt.numpy(),f2(tt).numpy(), \"-r\", label=\"f2\")\n",
    "    plt.plot(tt.numpy(),yt[:,1].numpy(),\"--r\", label=\"f2-pred\")\n",
    "    plt.plot(tt.numpy(),f3(tt).numpy(), \"-g\", label=\"f3\")\n",
    "    plt.plot(tt.numpy(),yt[:,2].numpy(),\"--g\", label=\"f3-pred\")\n",
    "    plt.plot(tt.numpy(),f4(tt).numpy(), \"-y\", label=\"f4\")\n",
    "    plt.plot(tt.numpy(),yt[:,3].numpy(),\"--y\", label=\"f4-pred\")\n",
    "    plt.plot(tt.numpy(),f5(tt).numpy(), \"-c\", label=\"f5\")\n",
    "    plt.plot(tt.numpy(),yt[:,4].numpy(),\"--c\", label=\"f5-pred\")\n",
    "    plt.plot(tt.numpy(),f6(tt).numpy(), \"-k\", label=\"f6\")\n",
    "    plt.plot(tt.numpy(),yt[:,5].numpy(),\"--k\", label=\"f6-pred\")\n",
    "    plt.plot(tt.numpy(),xt_enc[:,0].numpy(),\"--m\", label=\"x1\")\n",
    "    plt.plot(tt.numpy(),xt_enc[:,1].numpy(),\"-.m\", label=\"x2\")\n",
    "    plt.legend();            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
