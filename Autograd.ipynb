{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autograd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Experiments with `torch.autograd`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Python version is 3.7.0\n",
      "The Torch version is: 1.0.1.post2\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "print(\"The Python version is %s.%s.%s\" % sys.version_info[:3])\n",
    "import torch\n",
    "print(f'The Torch version is: {torch.__version__}')\n",
    "from torch import nn\n",
    "from torch.autograd import grad\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic examples : 1D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $f(w, z) = w+2z \\longrightarrow \\nabla f=(1,2)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dw,dz] = [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)   #  w.requires_grad = True\n",
    "z = torch.ones(1, requires_grad=True)   #  z.requires_grad = False\n",
    "f = w + 2*z     #  total.requires_grad = True\n",
    "# Automatic differentiation is straightforward:\n",
    "[dw,dz] = grad(f,[w,z])\n",
    "print(\"[dw,dz] = [{}, {}]\".format(dw.item(),dz.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Use of `backward()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dw,dz] = [1.0, 2.0]\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)   #  w.requires_grad = True\n",
    "z = torch.ones(1, requires_grad=True)   #  z.requires_grad = False\n",
    "f = w + 2*z     #  total.requires_grad = True\n",
    "f.backward()\n",
    "print(\"[dw,dz] = [{}, {}]\".format(w.grad.item(),z.grad.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### $f(w,z) =w+Sin(2z)\\longrightarrow \\nabla f(1,2)=(1.,-1.3072872417272239)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dw,dz] = [1.0, -1.3072872161865234]\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)   #  w.requires_grad = True\n",
    "z = torch.ones(1, requires_grad=True)*2   #  z.requires_grad = False\n",
    "f = w + torch.sin(2*z)     #  total.requires_grad = True\n",
    "# Automatic differentiation is straightforward:\n",
    "[dw,dz] = grad(f,[w,z])\n",
    "print(\"[dw,dz] = [{}, {}]\".format(dw.item(),dz.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Consider:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$g_1(w,z)=3w+\\cos(z), \\\\ g_2(w,z)=w^2+\\sin(2z),\\\\ f(w,z) = \\exp(g_1(w,z))+3 g_2(w,z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\longrightarrow \\nabla f(1,1)=(109.43202095367012,-31.508562530134782)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check this with Mathematica:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g1[w_, z_] := 3 w + Cos[z];\n",
    "g2[w_, z_] := w^2 + Sin[2 z];\n",
    "f[w_, z_] := Exp[g1[w, z]] + 3 g2[w, z];\n",
    "N[Grad[f[w, z], {w, z}]] /. {w -> 1, z -> 1}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[dw,dz] = [109.43202209472656, -31.508562088012695]\n"
     ]
    }
   ],
   "source": [
    "w = torch.ones(1, requires_grad=True)\n",
    "z = torch.ones(1, requires_grad=True)\n",
    "g1 = 3*w + torch.cos(z)\n",
    "g2 = torch.pow(w,2) + torch.sin(2*z)\n",
    "f = torch.exp(g1) + 3*g2 #  total.requires_grad = True\n",
    "f.backward()\n",
    "print(\"[dw,dz] = [{}, {}]\".format(w.grad.item(),z.grad.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic examples : 2D tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multivariate calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$G_1(W,Z)=3W+\\cos(Z), \\\\ G_2(W,Z)=W^2+\\sin(2Z),\\\\ F(W,Z) = \\exp(G_1(W,Z))+G_1(W,Z)G_2(W,Z)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\longrightarrow\n",
    "\\nabla_W F \\left(\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    " 1 & 0.5 \\\\\n",
    " 0.2 & 0.1 \\\\\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    " 1 & 0.5 \\\\\n",
    " 0.2 & 0.1 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\right)\n",
    "= \\left(\n",
    "\\begin{array}{cc}\n",
    " 109.231 & 15.7019 \\\\\n",
    " 9.28887 & 3.30879 \\\\\n",
    "\\end{array}\n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\longrightarrow\n",
    "\\nabla_Z F \\left(\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    " 1 & 0.5 \\\\\n",
    " 0.2 & 0.1 \\\\\n",
    "\\end{array}\n",
    "\\right),\n",
    "\\left(\n",
    "\\begin{array}{cc}\n",
    " 1 & 0.5 \\\\\n",
    " 0.2 & 0.1 \\\\\n",
    "\\end{array}\n",
    "\\right)\n",
    "\\right)\n",
    "= \\left(\n",
    "\\begin{array}{cc}\n",
    " -31.2551 & -0.557821 \\\\\n",
    " 4.13727 & 1.89057 \\\\\n",
    "\\end{array}\n",
    "\\right)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Check this with Mathematica:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "g1[w_,z_]:=3w+Cos[z];\n",
    "g2[w_,z_]:=w.w+Sin[2z];\n",
    "f[w_,z_]:=Norm[Exp[g1[w,z]]+g1[w,z].g2[w,z], \"Frobenius\"];\n",
    "N[f[{{Subscript[w, 11],Subscript[w, 12]},{Subscript[w, 21],Subscript[w, 22]}},{{Subscript[z, 11],Subscript[z, 12]},{Subscript[z, 21],Subscript[z, 22]}}]/.\n",
    "{Subscript[w, 11]->1,Subscript[w, 12]->0.5,\n",
    "Subscript[w, 21]->0.2,Subscript[w, 22]->0.1,\n",
    "Subscript[z, 11]->1,Subscript[z, 12]->0.5,\n",
    "Subscript[z, 21]->0.2,Subscript[z, 22]->0.1,\n",
    "(Abs^\\[Prime])[p_]->1}]\n",
    "N[Grad[f[{{Subscript[w, 11],Subscript[w, 12]},{Subscript[w, 21],Subscript[w, 22]}},{{Subscript[z, 11],Subscript[z, 12]},{Subscript[z, 21],Subscript[z, 22]}}],{Subscript[w, 11],Subscript[w, 12],Subscript[w, 21],Subscript[w, 22]}]] /.\n",
    "{Subscript[w, 11]->1,Subscript[w, 12]->0.5,\n",
    "Subscript[w, 21]->0.2,Subscript[w, 22]->0.1,\n",
    "Subscript[z, 11]->1,Subscript[z, 12]->0.5,\n",
    "Subscript[z, 21]->0.2,Subscript[z, 22]->0.1,\n",
    "(Abs^\\[Prime])[p_]->1}\n",
    "N[Grad[f[{{Subscript[w, 11],Subscript[w, 12]},{Subscript[w, 21],Subscript[w, 22]}},{{Subscript[z, 11],Subscript[z, 12]},{Subscript[z, 21],Subscript[z, 22]}}],{Subscript[z, 11],Subscript[z, 12],Subscript[z, 21],Subscript[z, 22]}]] /.\n",
    "{Subscript[w, 11]->1,Subscript[w, 12]->0.5,\n",
    "Subscript[w, 21]->0.2,Subscript[w, 22]->0.1,\n",
    "Subscript[z, 11]->1,Subscript[z, 12]->0.5,\n",
    "Subscript[z, 21]->0.2,Subscript[z, 22]->0.1,\n",
    "(Abs^\\[Prime])[p_]->1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F : 47.32332992553711\n",
      "dW = tensor([[109.2307,  15.7019],\n",
      "        [  9.2889,   3.3088]])\n",
      "dZ = tensor([[-31.2551,  -0.5578],\n",
      "        [  4.1373,   1.8906]])\n"
     ]
    }
   ],
   "source": [
    "W = torch.tensor([[1,0.5],[0.2,0.1]], requires_grad=True)\n",
    "Z = torch.tensor([[1,0.5],[0.2,0.1]], requires_grad=True)\n",
    "G1 = 3*W + torch.cos(Z)\n",
    "G2 = torch.matmul(W, W) + torch.sin(2*Z)\n",
    "F = torch.norm(torch.exp(G1) + torch.matmul(G1, G2),p = 2) #  total.requires_grad = True\n",
    "print(\"F : {}\".format(F))\n",
    "F.backward()\n",
    "print(\"dW = {}\".format(W.grad))\n",
    "print(\"dZ = {}\".format(Z.grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
